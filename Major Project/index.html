<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>AMME4710 Major Project</title>
    <link rel="stylesheet" href="https://arcms.w4v.es/Assets/Supreme/stylesheet.css">
    <link rel="stylesheet" href="./slide-styles.css">
    <script src = "./slides.js"></script>
    <script type = "module" src = "./signLive.js"></script>
  </head>
  <body>
    <p-slide>
      <h1>Introduction</h1>
      <span>Problem Statement - to offer a computer vision solution to the detection of American Sign Language (ASL) lettering through the aid of machine learning</span>
      <ul>
        <li>
          ASL has a larger number of datasets available than other sign languages (eg. AusLan)
        </li>
        <li>
          Can be used for applications such as live captioning, or as a learning resource
        </li>
        <li>
          Since sign language is a visual language, capturing and processing data through CV is logical
        </li>
      </ul>
    </p-slide>

    <p-slide>
      <h1>Lit Review</h1>
      <div style = "font-size: 0.9em;">
        <ul>
          <li>
            What other approaches exist
          </li>
          <li>
            State of the art stuff - architectures used, image processing used, results from these experiments
          </li>
        </ul>
        First attempts in the 1970s<br />
        <row-1 class = "smll">
          <div>
            Key Findings: (Kadous, 1970)
            <ul>
              <li>
                Used shallow based learning
              </li>
              <li>
                Predefined bounding box limits noise
              </li>
              <li>
                Use of sensor glove to provide depth information
              </li>
              <li>
                Achieved accuracy of 80%
              </li>
            </ul>
          </div>
          <div>
            Key Findings: (Rashad,2019)
            <ul>
              <li>
                Uses wearable motion sensors
              </li>
              <li>
                Attempts neural network and SVM approaches
              </li>
              <li>
                Achieved 93.7% accuracy with NN and 85.56% for SVM - why we chose SVM
              </li>
            </ul>
          </div>
          <div>
            Key Findings: (Omar,2019)
            <ul>
              <li>
                Discusses validity/benefit of thresholding and Gaussian filtering
              </li>
              <li>
                Removes info such as skin colour and shadows - same as edge detection
              </li>
            </ul>
          </div>
        </row-1>
        YOLOv3 is state of the art, having shown excellent performance on video data as it is fast
        Many attempts use gloves to assist machine learning however not practical in real life
      </div>
    </p-slide>

    <p-slide>
      <h1>Method - Preparing data</h1>
      <row-1>
        <div style = "width: 75%; font-size: 0.9em;">
        <ul>
          <li>
            Dataset description (where it’s from, what kinds of images, etc.)
          </li>
          <li>
            Subsampling
          </li>
          <li>
            Edge detection, preprocessing
          </li>
        </ul>
        Contains 27000 images subsampled to 2700 for training data 100 images for each possible ASL character
        <ul>
          <li>
            Images had various orientations of hands in different environments
          </li>
        </ul>
        Seperated into training and testing datasets
        Canny edge detection to help isolate hand and remove noise
        <ul>
          <li>
            Also helped to remove effects of shadows and skin tone
          </li>
          <li>
            Helps refine the data so the algorithm is limited to analyse what really matters - the shape of hand
          </li>
        </ul>

        <span style = "font-size: 0.7em">Dataset retrieved from: https://www.kaggle.com/datasets/lexset/synthetic-asl-alphabet</span>
      </div>
        <div class = "img-row">
          <row-1><img src = "./original1.png" /><img src = "./canny1.png" /></row-1>
          <row-1><img src = "./original2.png" /><img src = "./canny2.png" /></row-1>
          <row-1><img src = "./original3.png" /><img src = "./canny3.png" /></row-1>
        </div>
      </row-1>
      <!-- <img src = "filesystem:https://docs.google.com/persistent/docs/documents/1neRnNypoHdARpCgIy-gWV0t-lDmg6ykXurENhzFnBnI/image/1NfzezQj_eO4iWBVr0t3PC_7sixVNNFnAh0roq2q87Ns" /> -->
    </p-slide>
    <p-slide>
      <h1>Method - Processing Data (Cont.)</h1>
      About the dataset
      <ul>
        <li>
          100 images for 27 classes
        </li>
        <li>
          Hands in various orientations
        </li>
        <li>
          Seperated in training and testing datasets
        </li>
      </ul>
    </p-slide>
    <p-slide>
      <h1>Method - Training</h1>
      <ul>
        <li>
          Produced models on AlexNet, YOLO architectures
        </li>
        <li>
          AlexNet sequential model, YOLO residual model
        </li>
        <li>
          Played around with hyper parameters, image sizing issues
        </li>
        <li>
          Effect of convolutions, etc. How is the algorithm extracting features
        </li>
      </ul>
    </p-slide>
    <p-slide>
      <h1>Method - Implementation</h1>
      <ul>
        <li>
          Implementing on ubiquitous platform
        </li>
        <li>
          Real time accurate detection
        </li>
        <li>
          Captioning
        </li>
        <li>
          Detecting hands - predefined area where hands need to be placed
        </li>
        <li>
          Crop frame and predict with TensorFlow.js
        </li>
        <li>
          Localisation of web solution
        </li>
      </ul>
    </p-slide>
    <p-slide>
      <h1>Experimental setup, benchmarking and Results</h1>
      <row-1>
        <ul>
          <li>
            Data is collected → training set subsampled → edge detection to enhance relevant features and negate effect of environment/skin colour → pass through CNN inspired by AlexNet → tested against our own dataset
          </li>
          <li>
            Validation accuracy of 74.5%
          </li>
        </ul>
        <img style = "width: 50%" src = "./history.png" />
      </row-1>
    </p-slide>
    <p-slide>
      <h1>Live Demo</h1>
      <sign-live></sign-live>
    </p-slide>

    <p-slide>
      <h1>Discussion, Conclusion, Future Work</h1>
      <ul>
        <li>
          Implementing hand detection
        </li>
        <li>
          Using larger dataset, more epochs, more efficient algorithms, distributed system training
        </li>
        <li>
          Which letters were detected better
        </li>
        <li>
          Detection of words
        </li>
      </ul>
    </p-slide>
  </body>
</html>
