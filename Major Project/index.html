<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <title>AMME4710 Major Project</title>
    <link rel="stylesheet" href="https://arcms.w4v.es/Assets/Supreme/stylesheet.css">
    <link rel="stylesheet" href="./slide-styles.css">
    <script src="https://cdn.jsdelivr.net/gh/wallat/compiled-opencvjs/v4.2.0/opencv.js" async onload="window.cv = cv" type="text/javascript"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.0.0/dist/tf.min.js" onload = "window.tf = tf"></script>
    <script type = "module" src = "./slides.js"></script>
    <script type = "module" src = "./SignLive/signLive.js"></script>
  </head>
  <body>
    <p-slide>
      <h1>Introduction</h1>

      <span>Problem Statement - to offer a computer vision solution to the detection of American Sign Language (ASL) lettering through the aid of machine learning</span>
      <ul>
        <li>
          ASL has a larger number of datasets available than other sign languages (eg. AusLan)
        </li>
        <li>
          Can be used for applications such as live captioning, or as a learning resource
        </li>
        <li>
          Since sign language is a visual language, capturing and processing data through CV is logical
        </li>
      </ul>
    </p-slide>

    <p-slide>
      <h1>Literature Review</h1>

      <row-1>
        <div>
          Attempts at automated sign language detection in 1970
          <br />
          <br />
          Key Takeaways
          <ul>
            <li>
              Many attempts use hand sensors
            </li>
            <li>
              Image pre-processing using filtering and thresholding is effective
            </li>
            <li>
              Neural networks have produced the best results
            </li>
          </ul>
        </div>
        <table>
        <thead>
          <tr><td>Method</td> <td>Accuracy (%)</td></tr>
        </thead>
        <tbody>
          <tr><td>Instance Based Learning (Kadeous M, 1970)</td><td>80.00</td></tr>
          <tr><td>Decision Tree (Kadeous M, 1970)</td><td>55.00</td></tr>
          <tr><td>Support Vector Machine (Rashad S. et al. 2019)</td><td>96.41</td></tr>
          <tr><td>Neural Network (Rashad S. et al. 2019)</td><td>91.81</td></tr>
          <tr><td>k-Nearest Neighbour (Sharma A. et al. 2020)</td><td>95.81</td></tr>
          <tr><td>Neural Network (Raj R. et al. 2018)</td><td>99.00</td></tr>
        </tbody>
      </table>
      </row-1>
    </p-slide>

    <p-slide>
      <h1>Method - Preparing data</h1>

      <row-1>
        <div style = "width: 75%; font-size: 0.9em;">
          About the dataset
          <ul>
            <li>
              100 images for 27 classes
            </li>
            <li>
              Hands in various orientations
            </li>
            <li>
              Seperated in training and testing datasets
            </li>
          </ul>
          Canny Edge Detection
          <ul>
            <li>
              Removes skin tone, lighting effects, and background
            </li>
            <li>
              Refines the data
            </li>
          </ul>
          <span style = "font-size: 0.7em">Dataset retrieved from: https://www.kaggle.com/datasets/lexset/synthetic-asl-alphabet</span>
        </div>

        <!-- <div class = "img-row"> -->
          <figure style = "width:35%">
            <img width = "100%" src = "./Assets/all.png" />
          </figure>
        <!-- </div> -->
      </row-1>
    </p-slide>



    <p-slide>
      <h1>Method - Training</h1>

      <ul>
        <li>
          Machine learning model was made using Tensorflow Keras API
        </li>
        <li>
          Architecture based off of AlexNet, with modified input image sizes
        </li>
        <li>
          Trained for 5 epochs
        </li>
      </ul>
      <figure>

        <img src ="./Assets/fig5.png" width = "70%" style = "margin: 3% 15%"/>
      </figure>
    </p-slide>

    <p-slide>
      <h1>Method - Implementation</h1>

      <ul>
        <li>
          Implementing on ubiquitous platform
        </li>
        <li>
          Real time accurate detection
        </li>
        <li>
          Captioning
        </li>
        <li>
          Detecting hands - predefined area where hands need to be placed
        </li>
        <li>
          Crop frame and predict with TensorFlow.js
        </li>
        <li>
          Localisation of web solution
        </li>
      </ul>
    </p-slide>

    <p-slide>
      <h1>Experimental setup, benchmarking and Results</h1>

      <row-1>
        <ul>
          <li>
            Data is collected → training set subsampled → edge detection to enhance relevant features and negate effect of environment/skin colour → pass through CNN inspired by AlexNet → tested against our own dataset
          </li>
          <li>
            Validation accuracy of 74.5%
          </li>
        </ul>
        <figure style = "width: 60%">
          <img width = "100%" src = "./Assets/history.png" />
        </figure>
      </row-1>
    </p-slide>

    <p-slide>
      <h1>Live Demo</h1>

      <sign-live src = "./Model/model.json"></sign-live>
    </p-slide>

    <p-slide>
      <h1>Discussion, Conclusion, Future Work</h1>

      <ul>
        <li>
          Not a state-of-the-art solution, as there are more efficient models and methods (as per literature review)
        </li>
        <li>
          The accuracy can be improved if it is run for more epochs and if more data is used
        </li>
        <li>
          Future work could involve using better models and expanding the solution to interpreting words and sentences, which involve dynamic hand movements
        </li>
        <li>
          Experiment with alternate neural network architectures (i.e YOLOv3)
        </li>
      </ul>
    </p-slide>
  </body>
</html>
